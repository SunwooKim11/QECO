{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from MEC_Env import MEC\n",
        "from DDQN import DuelingDoubleDeepQNetwork\n",
        "from Config import Config\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "'''\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.mkdir(\"models\")\n",
        "else:\n",
        "    shutil.rmtree(\"models\")\n",
        "    os.mkdir(\"models\")\n",
        "'''\n",
        "\n",
        "def reward_fun(ue_comp_energy, ue_trans_energy, edge_comp_energy, ue_idle_energy, delay, max_delay, unfinish_task):\n",
        "    edge_energy  = next((e for e in edge_comp_energy if e != 0), 0)\n",
        "    idle_energy = next((e for e in ue_idle_energy if e != 0), 0)\n",
        "    penalty     = -max_delay*4\n",
        "    if unfinish_task == 1:\n",
        "        reward = penalty\n",
        "    else:\n",
        "        reward = 0\n",
        "    reward = reward - (ue_comp_energy + ue_trans_energy + edge_energy + idle_energy)\n",
        "    return reward\n",
        "\n",
        "def monitor_reward(ue_RL_list, episode):\n",
        "    episode_sum_reward = sum(sum(ue_RL.reward_store[episode]) for ue_RL in ue_RL_list)\n",
        "    avg_episode_sum_reward = episode_sum_reward / len(ue_RL_list)\n",
        "    print(f\"reward: {avg_episode_sum_reward}\")\n",
        "    return avg_episode_sum_reward\n",
        "\n",
        "def monitor_delay(ue_RL_list, episode):\n",
        "    delay_ue_list = [sum(ue_RL.delay_store[episode]) for ue_RL in ue_RL_list]\n",
        "    avg_delay_in_episode = sum(delay_ue_list) / len(delay_ue_list)\n",
        "    print(f\"delay: {avg_delay_in_episode}\")\n",
        "    return avg_delay_in_episode\n",
        "\n",
        "def monitor_energy(ue_RL_list, episode):\n",
        "    energy_ue_list = [sum(ue_RL.energy_store[episode]) for ue_RL in ue_RL_list]\n",
        "    avg_energy_in_episode = sum(energy_ue_list) / len(energy_ue_list)\n",
        "    print(f\"energy: {avg_energy_in_episode}\")\n",
        "    return avg_energy_in_episode\n",
        "\n",
        "def cal_reward(ue_RL_list):\n",
        "    total_sum_reward = 0\n",
        "    num_episodes = 0\n",
        "    for ue_num, ue_RL in enumerate(ue_RL_list):\n",
        "        print(\"________________________\")\n",
        "        print(\"ue_num:\", ue_num)\n",
        "        print(\"________________________\")\n",
        "        for episode, reward in enumerate(ue_RL.reward_store):\n",
        "            print(\"episode:\", episode)\n",
        "            reward_sum = sum(reward)\n",
        "            print(reward_sum)\n",
        "            total_sum_reward += reward_sum\n",
        "            num_episodes += 1\n",
        "    avg_reward = total_sum_reward / num_episodes\n",
        "    print(total_sum_reward, avg_reward)\n",
        "\n",
        "\n",
        "def train(ue_RL_list, NUM_EPISODE):\n",
        "    avg_reward_list = []\n",
        "    avg_reward_list_2 = []\n",
        "    avg_delay_list_in_episode = []\n",
        "    avg_energy_list_in_episode = []\n",
        "    num_task_drop_list_in_episode = []\n",
        "    RL_step = 0\n",
        "    a = 1\n",
        "\n",
        "    for episode in range(NUM_EPISODE):\n",
        "        print(\"episode  :\", episode)\n",
        "        print(\"epsilon  :\", ue_RL_list[0].epsilon)\n",
        "\n",
        "        # BITRATE ARRIVAL\n",
        "        bitarrive = np.random.uniform(env.min_arrive_size, env.max_arrive_size, size=[env.n_time, env.n_ue])\n",
        "        task_prob = env.task_arrive_prob\n",
        "        bitarrive = bitarrive * (np.random.uniform(0, 1, size=[env.n_time, env.n_ue]) < task_prob)\n",
        "        bitarrive[-env.max_delay:, :] = np.zeros([env.max_delay, env.n_ue])\n",
        "\n",
        "        # OBSERVATION MATRIX SETTING\n",
        "        history = list()\n",
        "        for time_index in range(env.n_time):\n",
        "            history.append(list())\n",
        "            for ue_index in range(env.n_ue):\n",
        "                tmp_dict = {'observation': np.zeros(env.n_features),\n",
        "                            'lstm': np.zeros(env.n_lstm_state),\n",
        "                            'action': np.nan,\n",
        "                            'observation_': np.zeros(env.n_features),\n",
        "                            'lstm_': np.zeros(env.n_lstm_state)}\n",
        "                history[time_index].append(tmp_dict)\n",
        "        reward_indicator = np.zeros([env.n_time, env.n_ue])\n",
        "\n",
        "        # INITIALIZE OBSERVATION\n",
        "        observation_all, lstm_state_all = env.reset(bitarrive)\n",
        "\n",
        "        # TRAIN DRL\n",
        "        while True:\n",
        "\n",
        "            # PERFORM ACTION\n",
        "            action_all = np.zeros([env.n_ue])\n",
        "            for ue_index in range(env.n_ue):\n",
        "                observation = np.squeeze(observation_all[ue_index, :])\n",
        "                if np.sum(observation) == 0:\n",
        "                    # if there is no task, action = 0 (also need to be stored)\n",
        "                    action_all[ue_index] = 0\n",
        "                else:\n",
        "                    action_all[ue_index] = ue_RL_list[ue_index].choose_action(observation)\n",
        "                    if observation[0] != 0:\n",
        "                        ue_RL_list[ue_index].do_store_action(episode, env.time_count, action_all[ue_index])\n",
        "\n",
        "            # OBSERVE THE NEXT STATE AND PROCESS DELAY (REWARD)\n",
        "            observation_all_, lstm_state_all_, done = env.step(action_all)\n",
        "\n",
        "            # should store this information in EACH time slot\n",
        "            for ue_index in range(env.n_ue):\n",
        "                ue_RL_list[ue_index].update_lstm(lstm_state_all_[ue_index,:])\n",
        "\n",
        "            # STORE MEMORY; STORE TRANSITION IF THE TASK PROCESS DELAY IS JUST UPDATED\n",
        "            for ue_index in range(env.n_ue):\n",
        "                obs = observation_all[ue_index, :]\n",
        "                lstm = np.squeeze(lstm_state_all[ue_index, :])\n",
        "                action = action_all[ue_index]\n",
        "                obs_ = observation_all_[ue_index]\n",
        "                lstm_ = np.squeeze(lstm_state_all_[ue_index,:])\n",
        "                history[env.time_count - 1][ue_index].update({\n",
        "                    'observation': obs,\n",
        "                    'lstm': lstm,\n",
        "                    'action': action,\n",
        "                    'observation_': obs_,\n",
        "                    'lstm_': lstm_\n",
        "                })\n",
        "\n",
        "                update_index = np.where((1 - reward_indicator[:,ue_index]) *env.process_delay[:,ue_index] > 0)[0]\n",
        "                if len(update_index) != 0:\n",
        "                    for time_index in update_index:\n",
        "                        reward = reward_fun(\n",
        "                            env.ue_comp_energy[time_index, ue_index],\n",
        "                            env.ue_tran_energy [time_index, ue_index],\n",
        "                            env.edge_comp_energy[time_index, ue_index],\n",
        "                            env.ue_idle_energy[time_index, ue_index],\n",
        "                            env.process_delay[time_index, ue_index],\n",
        "                            env.max_delay,\n",
        "                            env.unfinish_task[time_index, ue_index]\n",
        "                        )\n",
        "                        ue_RL_list[ue_index].store_transition(\n",
        "                            history[time_index][ue_index]['observation'],\n",
        "                            history[time_index][ue_index]['lstm'],\n",
        "                            history[time_index][ue_index]['action'],\n",
        "                            reward,\n",
        "                            history[time_index][ue_index]['observation_'],\n",
        "                            history[time_index][ue_index]['lstm_']\n",
        "                        )\n",
        "                        ue_RL_list[ue_index].do_store_reward(\n",
        "                            episode,\n",
        "                            time_index,\n",
        "                            reward\n",
        "                        )\n",
        "                        ue_RL_list[ue_index].do_store_delay(\n",
        "                            episode,\n",
        "                            time_index,\n",
        "                            env.process_delay[time_index, ue_index]\n",
        "                        )\n",
        "                        ue_RL_list[ue_index].do_store_energy(\n",
        "                            episode,\n",
        "                            time_index,\n",
        "                            env.ue_comp_energy[time_index, ue_index],\n",
        "                            env.ue_tran_energy [time_index, ue_index],\n",
        "                            env.edge_comp_energy[time_index, ue_index],\n",
        "                            env.ue_idle_energy[time_index, ue_index]\n",
        "                        )\n",
        "                        reward_indicator[time_index, ue_index] = 1\n",
        "\n",
        "\n",
        "            # ADD STEP (one step does not mean one store)\n",
        "            RL_step += 1\n",
        "\n",
        "            # UPDATE OBSERVATION\n",
        "            observation_all = observation_all_\n",
        "            lstm_state_all = lstm_state_all_\n",
        "\n",
        "            # CONTROL LEARNING START TIME AND FREQUENCY\n",
        "            if (RL_step > 200) and (RL_step % 10 == 0):\n",
        "                for ue in range(env.n_ue):\n",
        "                    ue_RL_list[ue].learn()\n",
        "\n",
        "            # GAME ENDS\n",
        "            if done:\n",
        "                for task in env.task_history:\n",
        "                    cmpl = drp = 0\n",
        "                    for t in task:\n",
        "                        d_states = t['d_state']\n",
        "                        if any(d < 0 for d in d_states):\n",
        "                            t['state'] = 'D'\n",
        "                            drp += 1\n",
        "                        elif all(d > 0 for d in d_states):\n",
        "                            t['state'] = 'C'\n",
        "                            cmpl += 1\n",
        "                full_complete_task = 0\n",
        "                full_drop_task = 0\n",
        "                complete_task = 0\n",
        "                drop_task = 0\n",
        "                for history in env.task_history:\n",
        "                    for task in history:\n",
        "                        if task['state'] == 'C':\n",
        "                            full_complete_task += 1\n",
        "                        elif task['state'] == 'D':\n",
        "                            full_drop_task += 1\n",
        "                        for component_state in task['d_state']:\n",
        "                            if component_state == 1:\n",
        "                                complete_task += 1\n",
        "                            elif component_state == -1:\n",
        "                                drop_task += 1\n",
        "                cnt = len(env.task_history) * len(env.task_history[0]) * env.n_component\n",
        "\n",
        "                print(\"++++++++++++++++++++++\")\n",
        "                print(\"drrop_rate   : \", full_drop_task/(cnt/env.n_component))\n",
        "                print(\"full_drrop   : \", full_drop_task)\n",
        "                print(\"full_complate: \", full_complete_task)\n",
        "                print(\"complete_task: \", complete_task)\n",
        "                print(\"drop_task:     \", drop_task)\n",
        "                print(\"++++++++++++++++++++++\")\n",
        "\n",
        "                if episode % 999 == 0 and episode != 0:\n",
        "                    os.mkdir(\"models\" + \"/\" + str(episode))\n",
        "                    for ue in range(env.n_ue):\n",
        "                        ue_RL_list[ue].saver.save(ue_RL_list[ue].sess, \"models/\" + str(episode) +'/'+ str(ue) + \"_X_model\" +'/model.ckpt', global_step=episode)\n",
        "\n",
        "                avg_reward_list.append(-(monitor_reward(ue_RL_list, episode)))\n",
        "                if episode % 10 == 0:\n",
        "                    avg_reward_list_2.append(sum(avg_reward_list[episode-10:episode])/10)\n",
        "                    avg_delay_list_in_episode.append(monitor_delay(ue_RL_list, episode))\n",
        "                    avg_energy_list_in_episode.append(monitor_energy(ue_RL_list, episode))\n",
        "                    total_drop = full_drop_task\n",
        "                    num_task_drop_list_in_episode.append(total_drop)\n",
        "\n",
        "                    # Plotting and saving figures\n",
        "                    '''\n",
        "                    fig, axs = plt.subplots(4, 1, figsize=(8, 16))\n",
        "                    axs[0].plot(avg_reward_list, '-')\n",
        "                    axs[0].set_ylabel('LSTM')\n",
        "                    axs[1].plot(avg_delay_list_in_episode, '-')\n",
        "                    axs[1].set_ylabel('r')\n",
        "                    axs[2].plot(avg_energy_list_in_episode, '-')\n",
        "                    axs[2].set_ylabel('r')\n",
        "                    axs[3].plot(num_task_drop_list_in_episode, '-')\n",
        "                    axs[3].set_ylabel('r')\n",
        "                    plt.savefig('figures.png')\n",
        "                    '''\n",
        "\n",
        "                    # Writing data to files\n",
        "                    '''\n",
        "                    data = [avg_reward_list, avg_delay_list_in_episode, avg_energy_list_in_episode, num_task_drop_list_in_episode]\n",
        "                    filenames = ['reward.txt', 'delay.txt', 'energy.txt', 'drop.txt']\n",
        "                    for i in range(len(data)):\n",
        "                        with open(filenames[i], 'w') as f:\n",
        "                            f.write('\\n'.join(str(x) for x in data[i]))\n",
        "                    '''\n",
        "                # Process energy\n",
        "                ue_bit_processed = sum(sum(env.ue_bit_processed))\n",
        "                ue_comp_energy = sum(sum(env.ue_comp_energy))\n",
        "\n",
        "                # Transmission energy\n",
        "                ue_bit_transmitted = sum(sum(env.ue_bit_transmitted))\n",
        "                ue_tran_energy = sum(sum(env.ue_tran_energy))\n",
        "\n",
        "                # edge energy\n",
        "                edge_bit_processed = sum(sum(env.edge_bit_processed))\n",
        "                edge_comp_energy = sum(sum(env.edge_comp_energy))\n",
        "                ue_idle_energy = sum(sum(env.ue_idle_energy))\n",
        "\n",
        "                # Print results\n",
        "                print(int(ue_bit_processed), ue_comp_energy, \"local\")\n",
        "                print(int(ue_bit_transmitted), ue_tran_energy, \"trans\")\n",
        "                print(int(sum(edge_bit_processed)),sum(edge_comp_energy), sum(ue_idle_energy), \"edge\")\n",
        "                print(\"_________________________________________________\")\n",
        "\n",
        "                break # Training Finished\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # GENERATE ENVIRONMENT\n",
        "    env = MEC(Config.N_UE, Config.N_EDGE, Config.N_TIME, Config.N_COMPONENT, Config.MAX_DELAY)\n",
        "\n",
        "    # GENERATE MULTIPLE CLASSES FOR RL\n",
        "    ue_RL_list = list()\n",
        "    for ue in range(Config.N_UE):\n",
        "        ue_RL_list.append(DuelingDoubleDeepQNetwork(env.n_actions, env.n_features, env.n_lstm_state, env.n_time,\n",
        "                                                    learning_rate       = Config.LEARNING_RATE,\n",
        "                                                    reward_decay        = Config.REWARD_DDECAY,\n",
        "                                                    e_greedy            = Config.E_GREEDY,\n",
        "                                                    replace_target_iter = Config.N_NETWORK_UPDATE,  # each 200 steps, update target net\n",
        "                                                    memory_size         = Config.MEMORY_SIZE,  # maximum of memory\n",
        "                                                    ))\n",
        "\n",
        "    # LOAD MODEL\n",
        "    '''\n",
        "    for ue in range(Config.N_UE):\n",
        "        ue_RL_list[ue].Initialize(ue_RL_list[ue].sess, ue)\n",
        "    '''\n",
        "\n",
        "    # TRAIN THE SYSTEM\n",
        "    train(ue_RL_list, Config.N_EPISODE)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}